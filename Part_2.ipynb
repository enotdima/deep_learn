{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFe273vKFgF-"
   },
   "source": [
    "# Homework 2, *part 2*\n",
    "### (60 points total)\n",
    "\n",
    "In this part, you will build a convolutional neural network (CNN) to solve (yet another) image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n",
    "\n",
    "**Unlike part 1**, you are now free to use the full power of PyTorch and its subpackages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKjuxXAgFgGA"
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "* This file.\n",
    "* A \"checkpoint file\" `\"checkpoint.pth\"` that contains your CNN's weights (you get them from `model.state_dict()`). Obtain it with `torch.save(..., \"checkpoint.pth\")`. When grading, we will load it to evaluate your accuracy.\n",
    "\n",
    "**Should you decide to put your `\"checkpoint.pth\"` on Google Drive, update (edit) the following cell with the link to it:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4kOWlcXFgGD"
   },
   "source": [
    "### [Dear TAs, I've put my \"checkpoint.pth\" on Google Drive, download it here](https://drive.google.com/file/d/1WLX-J-PbEicia-XTdVW-yzAO0OZ2K4bp/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S-WyYK8FgGF"
   },
   "source": [
    "## Grading\n",
    "\n",
    "* 9 points for reproducible training code and a filled report below.\n",
    "* 11 points for building a network that gets above 25% accuracy.\n",
    "* 4 points for using an **interactive** (please don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example Tensorboard ([with this library](https://github.com/lanpa/tensorboardX) and [an extra hack for Colab](https://stackoverflow.com/a/57791702)). In this notebook, insert screenshots of accuracy and loss plots (training and validation) over iterations/epochs/time.\n",
    "* 6 points for beating each of these accuracy milestones on the private **test** set:\n",
    "  * 30%\n",
    "  * 34%\n",
    "  * 38%\n",
    "  * 42%\n",
    "  * 46%\n",
    "  * 50%\n",
    "  \n",
    "*Private test set* means that you won't be able to evaluate your model on it. Rather, after you submit code and checkpoint, we will load your model and evaluate it on that test set ourselves, reporting your accuracy in a comment to the grade.\n",
    "\n",
    "Note that there is an important formatting requirement, see below near \"`DO_TRAIN = True`\".\n",
    "\n",
    "## Restrictions\n",
    "\n",
    "* No pretrained networks.\n",
    "* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n",
    "\n",
    "## Tips\n",
    "\n",
    "* **One change at a time**: never test several new things at once (unless you are super confident). Train a model, introduce one change, train again.\n",
    "* Google a lot: try to reinvent as few wheels as possible (unlike in part 1 of this assignment).\n",
    "* Use GPU.\n",
    "* Use regularization: L2, batch normalization, dropout, data augmentation...\n",
    "* Pay much attention to accuracy and loss graphs (e.g. in Tensorboard). Track failures early, stop bad experiments early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2qxvLGdFgGH"
   },
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2020 -- Home Assignment 2\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_6veP_5FgGS"
   },
   "source": [
    "The below cell puts training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njk50aDoFgGT"
   },
   "outputs": [],
   "source": [
    "# Extract the dataset into the current directory\n",
    "if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2020.zip', 'r') as archive:\n",
    "        archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1mtHCQcFgGb"
   },
   "source": [
    "**You are required** to format your notebook cells so that `Run All` on a fresh notebook:\n",
    "* trains your model from scratch, if `DO_TRAIN is True`;\n",
    "* loads your trained model from `\"./checkpoint.pth\"`, then **computes** and prints its validation accuracy, if `DO_TRAIN is False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QvH36LxFgGc"
   },
   "outputs": [],
   "source": [
    "DO_TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rgg4D0zSFgGi"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-148f73c3092afe91\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-148f73c3092afe91\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "%tensorboard --logdir {\"./logs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = 'logs/model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, size, criterion, optimizer, scheduler, epochs):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "\n",
    "        for stage in ['train', 'val']:\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            curr_loss = 0\n",
    "            if stage == 'train':\n",
    "                model.train()  \n",
    "                for (inputs, labels) in data['train']:\n",
    "                    i += 1\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "    \n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        outputs = model(inputs)\n",
    "                        prob, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    curr_loss = loss.item() * inputs.size(0)\n",
    "                    running_loss += curr_loss\n",
    "                    writer.add_scalar('Train loss', curr_loss, global_step = i)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                train_acc = running_corrects.double() / size[0]                \n",
    "                train_loss = running_loss / size[0]\n",
    "\n",
    "                print(train_loss, train_acc)\n",
    "            else:\n",
    "                model.eval()   \n",
    "                for (inputs, labels) in data['val']:\n",
    "                    j += 1\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "    \n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = model(inputs)\n",
    "                        prob, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    curr_loss = loss.item() * inputs.size(0)\n",
    "                    running_loss += curr_loss\n",
    "                    writer.add_scalar('Val loss', curr_loss, global_step = j)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                eval_acc = running_corrects.double() / size[1]\n",
    "                eval_loss = running_loss / size[1]\n",
    "                print(eval_loss, eval_acc)\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        transforms.RandomResizedCrop(size=(64, 64), scale = (0.7, 1.0)),\n",
    "        #transforms.CenterCrop(50),\n",
    "        #transforms.Resize((64,64), interpolation=2),\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(35),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join('tiny-imagenet-200', x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=100, shuffle=True, num_workers=64)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = (len(image_datasets['train']), len(image_datasets['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "4.683117496490478 tensor(0.0576, device='cuda:0', dtype=torch.float64)\n",
      "4.4236318445205685 tensor(0.0773, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 2/50\n",
      "----------\n",
      "4.34192391037941 tensor(0.0906, device='cuda:0', dtype=torch.float64)\n",
      "4.474054937362671 tensor(0.0781, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 3/50\n",
      "----------\n",
      "4.270411863327026 tensor(0.1009, device='cuda:0', dtype=torch.float64)\n",
      "4.762742962837219 tensor(0.0619, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 4/50\n",
      "----------\n",
      "4.2413978748321535 tensor(0.1047, device='cuda:0', dtype=torch.float64)\n",
      "4.526197094917297 tensor(0.0712, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 5/50\n",
      "----------\n",
      "4.218556240558624 tensor(0.1089, device='cuda:0', dtype=torch.float64)\n",
      "4.522344665527344 tensor(0.0742, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 6/50\n",
      "----------\n",
      "4.210513427734375 tensor(0.1087, device='cuda:0', dtype=torch.float64)\n",
      "4.955681881904602 tensor(0.0550, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 7/50\n",
      "----------\n",
      "4.193389797925949 tensor(0.1132, device='cuda:0', dtype=torch.float64)\n",
      "4.766452345848084 tensor(0.0680, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 8/50\n",
      "----------\n",
      "4.19523579120636 tensor(0.1124, device='cuda:0', dtype=torch.float64)\n",
      "4.789442081451416 tensor(0.0637, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 9/50\n",
      "----------\n",
      "4.193890756607056 tensor(0.1120, device='cuda:0', dtype=torch.float64)\n",
      "4.255211095809937 tensor(0.0991, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 10/50\n",
      "----------\n",
      "4.1923014960289 tensor(0.1143, device='cuda:0', dtype=torch.float64)\n",
      "4.661909990310669 tensor(0.0772, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 11/50\n",
      "----------\n",
      "4.189822399616242 tensor(0.1139, device='cuda:0', dtype=torch.float64)\n",
      "4.443121209144592 tensor(0.0842, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 12/50\n",
      "----------\n",
      "4.184865514039993 tensor(0.1136, device='cuda:0', dtype=torch.float64)\n",
      "4.493732509613037 tensor(0.0859, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 13/50\n",
      "----------\n",
      "4.191274358987808 tensor(0.1126, device='cuda:0', dtype=torch.float64)\n",
      "4.556676955223083 tensor(0.0710, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 14/50\n",
      "----------\n",
      "3.7538523058891298 tensor(0.1810, device='cuda:0', dtype=torch.float64)\n",
      "3.555557842254639 tensor(0.2001, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 15/50\n",
      "----------\n",
      "3.5946240723133087 tensor(0.2044, device='cuda:0', dtype=torch.float64)\n",
      "3.7504371905326845 tensor(0.1751, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 16/50\n",
      "----------\n",
      "3.536042820453644 tensor(0.2145, device='cuda:0', dtype=torch.float64)\n",
      "3.550191900730133 tensor(0.2075, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 17/50\n",
      "----------\n",
      "3.471793396472931 tensor(0.2260, device='cuda:0', dtype=torch.float64)\n",
      "3.6721516966819765 tensor(0.1927, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 18/50\n",
      "----------\n",
      "3.4114121992588045 tensor(0.2372, device='cuda:0', dtype=torch.float64)\n",
      "3.7057650828361512 tensor(0.1902, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 19/50\n",
      "----------\n",
      "3.3612899243831635 tensor(0.2472, device='cuda:0', dtype=torch.float64)\n",
      "3.525054588317871 tensor(0.2201, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 20/50\n",
      "----------\n",
      "3.3162618141174316 tensor(0.2547, device='cuda:0', dtype=torch.float64)\n",
      "3.3064533948898314 tensor(0.2448, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 21/50\n",
      "----------\n",
      "3.040212646961212 tensor(0.3106, device='cuda:0', dtype=torch.float64)\n",
      "2.9807252740859984 tensor(0.3142, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 22/50\n",
      "----------\n",
      "2.949824743270874 tensor(0.3292, device='cuda:0', dtype=torch.float64)\n",
      "3.006810448169708 tensor(0.3089, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 23/50\n",
      "----------\n",
      "2.8982593660354614 tensor(0.3389, device='cuda:0', dtype=torch.float64)\n",
      "3.0482904744148254 tensor(0.2995, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 24/50\n",
      "----------\n",
      "2.8597648499011994 tensor(0.3461, device='cuda:0', dtype=torch.float64)\n",
      "2.9390283465385436 tensor(0.3223, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 25/50\n",
      "----------\n",
      "2.821442829847336 tensor(0.3531, device='cuda:0', dtype=torch.float64)\n",
      "2.8928325891494753 tensor(0.3347, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 26/50\n",
      "----------\n",
      "2.793833729505539 tensor(0.3586, device='cuda:0', dtype=torch.float64)\n",
      "2.8359531283378603 tensor(0.3459, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 27/50\n",
      "----------\n",
      "2.7617760293483733 tensor(0.3646, device='cuda:0', dtype=torch.float64)\n",
      "2.785288329124451 tensor(0.3526, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 28/50\n",
      "----------\n",
      "2.7271229174137117 tensor(0.3734, device='cuda:0', dtype=torch.float64)\n",
      "2.788705413341522 tensor(0.3522, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 29/50\n",
      "----------\n",
      "2.7014186527729036 tensor(0.3761, device='cuda:0', dtype=torch.float64)\n",
      "3.004333829879761 tensor(0.3122, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 30/50\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-f7ff5d9122f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmilestones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-035a24b1f8bc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data, size, criterion, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurr_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18()\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 200)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.05, momentum=0.9, weight_decay=0.005, nesterov = True)\n",
    "exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[25, 39], gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(64, 64), scale = (0.6, 1.0)),\n",
    "        #transforms.CenterCrop(50),\n",
    "        #transforms.Resize((64,64), interpolation=2),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9n7DyGcFgGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "4.547195558309555 tensor(0.0756, device='cuda:0', dtype=torch.float64)\n",
      "4.901333947181701 tensor(0.0550, device='cuda:0', dtype=torch.float64)\n",
      "Epoch 2/50\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join('tiny-imagenet-200', x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=100, shuffle=True, num_workers=64)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "sizes = (len(image_datasets['train']), len(image_datasets['val']))\n",
    "\n",
    "if DO_TRAIN:\n",
    "    model = models.densenet121()\n",
    "    num = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num, 200)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.07, momentum=0.9, nesterov = True, weight_decay = 0.001)\n",
    "    multi_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[25, 35], gamma=0.3)\n",
    "    \n",
    "    model = train_model(model, dataloader, sizes, criterion, optimizer, multi_scheduler, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHFdDYv8FgGx"
   },
   "source": [
    "## Load and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join('tiny-imagenet-200', x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_eval = torch.utils.data.DataLoader(image_datasets['val'], batch_size=100,\n",
    "                                             shuffle=True, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-YNjqEMFgGy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51.4600, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Your code here (load the model from \"./checkpoint.pth\")\n",
    "# Please use `torch.load(\"checkpoint.pth\", map_location='cpu')`\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.densenet121()\n",
    "num = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num, 200)\n",
    "model.load_state_dict(torch.load('model_dense_27.pt'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "val_sum = 0\n",
    "for img, labels in img_eval:\n",
    "    img = img.to(device)\n",
    "    labels = labels.to(device)\n",
    "    _, pred_labels = torch.max(model(img), 1)\n",
    "    val_sum += torch.sum(pred_labels == labels.data)\n",
    "\n",
    "val_accuracy = (val_sum.double() / 10000)*100\n",
    "\n",
    "\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYKCk2rnFgG2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 51.46%\n"
     ]
    }
   ],
   "source": [
    "#val_accuracy = # Your code here\n",
    "assert 0 <= val_accuracy <= 100\n",
    "print(\"Validation accuracy: %.2f%%\" % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crMQ_SvcFgG7"
   },
   "source": [
    "# Report\n",
    "\n",
    "Below, please mention:\n",
    "\n",
    "* A brief history of tweaks and improvements.\n",
    "* Which network architectures have you tried? What is the final one and why?\n",
    "* What is the training method (batch size, optimization algorithm, number of iterations, ...) and why?\n",
    "* Which techniques have you tried to prevent overfitting? What were their effects? Which of them worked well?\n",
    "* Any other insights you learned.\n",
    "\n",
    "For example, start with:\n",
    "\n",
    "\"I have analyzed these and those conference papers/sources/blog posts. \\\n",
    "I tried this and that to adapt them to my problem. \\\n",
    "The conclusions this task taught me are ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the various sources and papers () I get the knowledge, that standart ResNet architectures without pretraining could give descent results, if you will select parameters carefully.\n",
    "\n",
    "I tried ResNet-18 and ResNet-34 architectures as a baseline, which gave me somewhat good results of 0.35 validation accuracy, and then they were overfitting and plateauing. \n",
    "\n",
    "Trying to solve this problem, I added various augmentations (crops, flips, rotations) and introduced weight decay to my SGD optimizer. It helped to make overfitting smaller. Playing with optimizer also gave me an idea of gradually reducing Learning rate, which gave some advantage in final validation accuracy of resnet-18, the best one was about 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet loss](pict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that point I read about another type of nets used in image classification - DenseNets. Even though the learning time was much bigger, it gave better results initially. Using the same tricks that I used with ResNets I has made progress to validation accuracy 0.5. The last succesful decision was to make my SGD Nesteriv SGD, which gave the final boost and made the final result 0.514 val accuracy with small overfittiing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above other techniques, I also tried Cyclic Learning rate, which was shown to be not stable in case of CNN's, I also tried different starting learning rates, but in my experience quite big learning rates (1e-2) gave better results then smaller ones. The best results was acquired typically about 25-30 epochs. \n",
    "\n",
    "I experimented with batch sizes, but in my case it did not give much improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this task taught me that even standard structures could give relatively good results if you use right set of parameters and augmentations. It also learned me to prevent overfitting and believe in myself :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
